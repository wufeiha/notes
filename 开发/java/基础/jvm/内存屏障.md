## 引言

- 原理：CPU提供了原子操作、关中断、锁内存总线,内存屏障等机制；OS基于这几个CPU硬件机制，就能够实现锁；再基于锁，就能够实现各种各样的同步机制（信号量、消息、Barrier等等等等）。

- <font color=red>所有的同步操作最基础的理论就是原子操作。内存屏障,锁都是为了保证在不同的平台或者是CPU类型下的原子操作。</font>

- 内存屏障<font color=orange>用于控制特定条件下的重排序和内存可见性问题。</font> Java编译器也会根据内存屏障的规则禁止重排序。 有的处理器的重排序规则较严，无需内存屏障也能很好的工作，Java编译器会在这种情况下不放置内存屏障

## 多线程开发需要注意的问题

- **原子性**
   即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执。
   
- **可见性**
   可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。
   
- **有序性**（重排序）
   即程序执行的顺序按照代码的先后顺序执行。
   
   ![img](https://i.loli.net/2021/06/19/byR3ltGe5VUoSKZ.png)

## 内存屏障种类

### 硬件层

硬件层的内存屏障分为两种：**Load Barrier （读屏障）** 和 **Store Barrier（写屏障）及 Full Barrier（全屏障）** 是读屏障和写屏障的合集。内存屏障有两个作用：

- **阻止**屏障两侧的指令重排序；

- 写屏障：强制**把写缓冲区/高速缓存**中的脏数据等**写回主内存**（刷新store buffer），读屏障：将缓冲区/高速缓存中相应的数据失效（刷新invalidate queue）。

  [store buffer、invalidate queue与内存屏障的关系](https://zhuanlan.zhihu.com/p/184912992)

### jvm层

java的内存屏障通常所谓的四种即，LoadLoad（LL）,StoreStore（SS）,LoadStore（LS）,StoreLoad（SL）

 - **LoadLoad（LL）屏障**：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。

 - **StoreStore（SS）屏障**：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。

 - **LoadStore（LS）屏障**：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。

 - **StoreLoad（SL）屏障**：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，<font color=red>这个屏障是个万能屏障，兼具其它三种内存屏障的功能。</font>

   > **为什么这一堆 Barrier 里 `StoreLoad` 最重？**
   >
   > 所谓的重实际就是跟内存交互次数，交互越多延迟越大，也就是越重。`StoreStore`， `LoadLoad` 两个都不提了，因为它俩要么只限制读，要么只限制写，也即只有一次内存交互。只有 `LoadStore` 和 `StoreLoad` 看上去有可能对读写都有限制。但 `LoadStore` 里实际限制的更多的是读，即 Load 数据进来，它并不对最后的 Store 存出去数据的可见性有要求，只是说 Store 不能重排到 Load 之前。而反观 `StoreLoad`，它是说不能让 Load 重排到 Store 之前，这么一来得要求在 Load 操作前刷写 Store Buffer 到内存。不去刷 Store Buffer 的话，就可能导致先执行了读取操作。而数据一旦刷写出去，别的 CPU 就能看到，看到之后可能就会修改下一步 Load 操作的内存导致 Load 操作的内存所在 Cache Line 无效。如果允许 Load 操作从一个可能被 Invalidate 的 Cache Line 里读数据，则表示 Load 从实际意义上来说被重排到了 Store 之前，因为这个数据可能是 Store 前就在 Cache 中的，相当于读操作提前了。为了避免这种事发生，Store 完成后一定要去处理 Invalidate Queue，去判断自己 Load 操作的内存所在 Cache Line 是否被设置为无效。这么一来为了满足 `StoreLoad` 的要求，一方面要刷 Store Buffer，一方面要处理 Invalidate Queue，则最差情况下会有两次内存操作，读写分别一次，所以它最重
   >
   > **`StoreLoad` 为什么能实现其它 Barrier 的功能？**
   >
   > 这个也是从前一个问题结果能看出来的。`StoreLoad` 因为对读写操作均有要求，所以它能实现其它 Barrier 的功能。其它 Barrier 都是只对读写之中的一个方面有要求。
   >
   > 不过这四个 Barrier 只是 Java 为了跨平台而设计出来的，实际上根据 CPU 的不同，对应 CPU 平台上的 JVM 可能可以优化掉一些 Barrier。比如很多 CPU 在读写同一个变量的时候能保证它连续操作的顺序性，那就不用加 Barrier 了。比如 `Load x; Load x.field` 读 x 再读 x 下面某个 field，如果访问同一个内存 CPU 能保证顺序性，两次读取之间的 Barrier 就不再需要了，根据字节码编译得到的汇编指令中，本来应该插入 Barrier 的地方会被替换为 `nop`，即空操作。在 x86 上，实际只有 `StoreLoad` 这一个 Barrier 是有效的，x86 上没有 Invalidate Queue，每次 Store 数据又都会去 Store Buffer 排队，所以 `StoreStore`， `LoadLoad` 都不需要。x86 又能保证 Store 操作都会走 Store Buffer 异步刷写，Store 不会被重排到 Load 之前，`LoadStore` 也是不需要的。只剩下一个 `StoreLoad` Barrier 在 x86 平台的 JVM 上被使用。

## java中的使用

### UNSAFE.putOrderedObject

UNSAFE.putOrderedObject，避免了写写指令重排序，但不保证内存可见性（个人理解，只加SS屏障，未加SL屏障，因此未刷新invalidate queue，旧的cache line还未失效）

### volatile

volatile的内存屏障策略非常严格保守，非常**悲观**且**毫无安全感**的心态：

- 在每个volatile写操作前插入**StoreStore（SS）屏障**，在写操作后插入StoreLoad屏障；

- 在每个volatile读操作前插入**LoadLoad（LL）屏障**，在读操作后插入LoadStore屏障；

  ![img](https://i.loli.net/2021/06/20/ESq4uc7QydCBI13.png)

### final

1、新建对象过程中，构造体中对final域的初始化写入和这个对象赋值给其他引用变量，这两个操作不能重排序；

2、初次读包含final域的对象引用和读取这个final域，这两个操作不能重排序；（意思就是先赋值引用，再调用final值）
 总之上面规则的意思可以这样理解：必需保证一个对象的所有final域被写入完毕后才能引用和读取。这也是内存屏障的起的作用：
     1、写final域：在编译器写final域完毕，构造体结束之前，会插入一个StoreStore屏障，保证前面的对final写入对其他线程/CPU可见，并阻止重排序。
     2、读final域：在上述规则2中，两步操作不能重排序的机理就是在读final域前插入了LoadLoad屏障。

## HappenBefore原则

   `HappenBefore解决的是可见性问题`
    定义：前一个操作的结果对于后续操作是可见的。在 JMM 中，如果一个操作执行的结果需要对另一个操作课件，那么这两个操作必须要存在 happens-before 关系。这两个操作可以是同一个线程，也可以是不同的线程。

**JMM 中有哪些方法建立 happen-before 规则：**

- 1、**as-if-serial 规则（程序顺序执行）**：单个线程中的代码顺序不管怎么重排序，对于结果来说是不变的。
- 2、**volatile 变量规则**，对于 volatile 修饰的变量的写的操作， 一定 happen-before 后续对于 volatile 变量的读操作;
- 3、**监视器锁规则（monitor lock rule）**：对一个监视器的解锁，happens-before于随后对这个监视器的加锁。
- 4、**传递性规则**：如果A happens-before B，且B happens-before C，那么A happens-before C。
- 5、**start 规则**：如果线程 A 执行操作 ThreadB.start(),那么线程 A 的 ThreadB.start()操作 happens-before 线程 B 中的任意操作。
- 6、**join 规则**：如果线程 A 执行操作 ThreadB.join()并成功返回，那么线程 B 中的任意操作 happens-before 于线程 A 从 ThreadB.join()操作成功返回。

这几条规则单独看上去没有什么厉害的地方，这些规则从来都不是单独出现的。。。综合运用效果？



```java
class VolatileExample {
    int a = 0;
    volatile boolean flag = false;

    public void writer() {
        a = 1;           //1
        flag = true;     //2
    }

    public void reader() {
        if (flag) {       //3
            int i = a;    //4
            ...
        }
    }
}
```

​    **假设**线程A执行writer()方法之**后**，线程B执行reader()方法，那么线程B执行4的时候一定能看到线程A写入的值吗？注意，a不是volatile变量。
​    答案是**肯定的**。因为根据happens-before规则，我们可以得到如下关系：
​    根据**程序顺序规则**，1 happens-before 2；3 happens-before 4。
​    根据**volatile规则**，2 happens-before 3。
​    根据**传递性规则**，1 happens-before 4。
​    因此，综合运用**程序顺序规则、volatile规则及传递性规则**，我们可以得到1 happens-before 4，即线程B在执行4的时候一定能看到A写入的值。